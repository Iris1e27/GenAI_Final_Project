{"_default": {"1": {"file": "arXiv Fulltext PDF:C\\:\\\\Users\\\\Jingtian Zhang\\\\Zotero\\\\storage\\\\WVE859LQ\\\\Zeng \u7b49 - 2023 - AgentTuning Enabling Generalized Agent Abilities .pdf:application/pdf;arXiv.org Snapshot:C\\:\\\\Users\\\\Jingtian Zhang\\\\Zotero\\\\storage\\\\NE6ZG26B\\\\2310.html:text/html", "annote": "Comment: 31 pages", "keywords": "Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning", "note": "arXiv:2310.12823 [cs]", "year": "2023", "month": "October", "author": "Zeng, Aohan and Liu, Mingdao and Lu, Rui and Wang, Bowen and Liu, Xiao and Dong, Yuxiao and Tang, Jie", "publisher": "arXiv", "urldate": "2023-10-26", "abstract": "Open large language models (LLMs) with great performance in various tasks have significantly advanced the development of LLMs. However, they are far inferior to commercial models such as ChatGPT and GPT-4 when acting as agents to tackle complex tasks in the real world. These agent tasks employ LLMs as the central controller responsible for planning, memorization, and tool utilization, necessitating both fine-grained prompting methods and robust LLMs to achieve satisfactory performance. Though many prompting methods have been proposed to complete particular agent tasks, there is lack of research focusing on improving the agent capabilities of LLMs themselves without compromising their general abilities. In this work, we present AgentTuning, a simple and general method to enhance the agent abilities of LLMs while maintaining their general LLM capabilities. We construct AgentInstruct, a lightweight instruction-tuning dataset containing high-quality interaction trajectories. We employ a hybrid instruction-tuning strategy by combining AgentInstruct with open-source instructions from general domains. AgentTuning is used to instruction-tune the Llama 2 series, resulting in AgentLM. Our evaluations show that AgentTuning enables LLMs' agent capabilities without compromising general abilities. The AgentLM-70B is comparable to GPT-3.5-turbo on unseen agent tasks, demonstrating generalized agent capabilities. We open source the AgentInstruct and AgentLM-7B, 13B, and 70B models at https://github.com/THUDM/AgentTuning, serving open and powerful alternatives to commercial LLMs for agent tasks.", "url": "http://arxiv.org/abs/2310.12823", "shorttitle": "{AgentTuning}", "title": "AgentTuning: Enabling Generalized Agent Abilities for LLMs", "ENTRYTYPE": "misc", "ID": "zeng_agenttuning_2023"}, "2": {"file": "arXiv Fulltext PDF:C\\:\\\\Users\\\\Jingtian Zhang\\\\Zotero\\\\storage\\\\23QCPJGZ\\\\Hong \u7b49 - 2023 - MetaGPT Meta Programming for Multi-Agent Collabor.pdf:application/pdf;arXiv.org Snapshot:C\\:\\\\Users\\\\Jingtian Zhang\\\\Zotero\\\\storage\\\\XKBL2ZS7\\\\2308.html:text/html", "keywords": "Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems", "note": "arXiv:2308.00352 [cs]", "year": "2023", "month": "August", "author": "Hong, Sirui and Zheng, Xiawu and Chen, Jonathan and Cheng, Yuheng and Wang, Jinlin and Zhang, Ceyao and Wang, Zili and Yau, Steven Ka Shing and Lin, Zijuan and Zhou, Liyang and Ran, Chenyu and Xiao, Lingfeng and Wu, Chenglin", "publisher": "arXiv", "urldate": "2023-10-26", "abstract": "Recently, remarkable progress has been made in automated task-solving through the use of multi-agent driven by large language models (LLMs). However, existing LLM-based multi-agent works primarily focus on solving simple dialogue tasks, and complex tasks are rarely studied, mainly due to the LLM hallucination problem. This type of hallucination becomes cascading when naively chaining multiple intelligent agents, resulting in a failure to effectively address complex problems. Therefore, we introduce MetaGPT, an innovative framework that incorporates efficient human workflows as a meta programming approach into LLM-based multi-agent collaboration. Specifically, MetaGPT encodes Standardized Operating Procedures (SOPs) into prompts to enhance structured coordination. Subsequently, it mandates modular outputs, empowering agents with domain expertise comparable to human professionals, to validate outputs and minimize compounded errors. In this way, MetaGPT leverages the assembly line paradigm to assign diverse roles to various agents, thereby establishing a framework that can effectively and cohesively deconstruct complex multi-agent collaborative problems. Our experiments on collaborative software engineering benchmarks demonstrate that MetaGPT generates more coherent and correct solutions compared to existing chat-based multi-agent systems. This highlights the potential of integrating human domain knowledge into multi-agent systems, thereby creating new opportunities to tackle complex real-world challenges. The GitHub repository of this project is publicly available on:https://github.com/geekan/MetaGPT.", "url": "http://arxiv.org/abs/2308.00352", "shorttitle": "{MetaGPT}", "title": "MetaGPT: Meta Programming for Multi-Agent Collaborative Framework", "ENTRYTYPE": "misc", "ID": "hong_metagpt_2023"}, "3": {"file": "arXiv Fulltext PDF:C\\:\\\\Users\\\\Jingtian Zhang\\\\Zotero\\\\storage\\\\PD7NG9M9\\\\Wu \u7b49 - 2022 - PromptChainer Chaining Large Language Model Promp.pdf:application/pdf;arXiv.org Snapshot:C\\:\\\\Users\\\\Jingtian Zhang\\\\Zotero\\\\storage\\\\26RPNDB8\\\\2203.html:text/html", "annote": "Comment: CHI LBW 2022", "keywords": "Computer Science - Human-Computer Interaction", "note": "arXiv:2203.06566 [cs]", "year": "2022", "month": "March", "author": "Wu, Tongshuang and Jiang, Ellen and Donsbach, Aaron and Gray, Jeff and Molina, Alejandra and Terry, Michael and Cai, Carrie J.", "publisher": "arXiv", "urldate": "2023-10-26", "abstract": "While LLMs can effectively help prototype single ML functionalities, many real-world applications involve complex tasks that cannot be easily handled via a single run of an LLM. Recent work has found that chaining multiple LLM runs together (with the output of one step being the input to the next) can help users accomplish these more complex tasks, and in a way that is perceived to be more transparent and controllable. However, it remains unknown what users need when authoring their own LLM chains -- a key step for lowering the barriers for non-AI-experts to prototype AI-infused applications. In this work, we explore the LLM chain authoring process. We conclude from pilot studies find that chaining requires careful scaffolding for transforming intermediate node outputs, as well as debugging the chain at multiple granularities; to help with these needs, we designed PromptChainer, an interactive interface for visually programming chains. Through case studies with four people, we show that PromptChainer supports building prototypes for a range of applications, and conclude with open questions on scaling chains to complex tasks, and supporting low-fi chain prototyping.", "url": "http://arxiv.org/abs/2203.06566", "shorttitle": "{PromptChainer}", "title": "PromptChainer: Chaining Large Language Model Prompts through Visual Programming", "ENTRYTYPE": "misc", "ID": "wu_promptchainer_2022"}, "4": {"file": "arXiv Fulltext PDF:C\\:\\\\Users\\\\Jingtian Zhang\\\\Zotero\\\\storage\\\\YMEYJCL3\\\\Sachdeva \u548c McAuley - 2023 - Data Distillation A Survey.pdf:application/pdf;arXiv.org Snapshot:C\\:\\\\Users\\\\Jingtian Zhang\\\\Zotero\\\\storage\\\\GDYT9REA\\\\2301.html:text/html", "annote": "Comment: Accepted at TMLR '23. 21 pages, 4 figures", "keywords": "Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Information Retrieval", "note": "arXiv:2301.04272 [cs]", "year": "2023", "month": "September", "author": "Sachdeva, Noveen and McAuley, Julian", "publisher": "arXiv", "urldate": "2023-10-26", "abstract": "The popularity of deep learning has led to the curation of a vast number of massive and multifarious datasets. Despite having close-to-human performance on individual tasks, training parameter-hungry models on large datasets poses multi-faceted problems such as (a) high model-training time; (b) slow research iteration; and (c) poor eco-sustainability. As an alternative, data distillation approaches aim to synthesize terse data summaries, which can serve as effective drop-in replacements of the original dataset for scenarios like model training, inference, architecture search, etc. In this survey, we present a formal framework for data distillation, along with providing a detailed taxonomy of existing approaches. Additionally, we cover data distillation approaches for different data modalities, namely images, graphs, and user-item interactions (recommender systems), while also identifying current challenges and future research directions.", "url": "http://arxiv.org/abs/2301.04272", "shorttitle": "Data {Distillation}", "title": "Data Distillation: A Survey", "ENTRYTYPE": "misc", "ID": "sachdeva_data_2023"}, "5": {"file": "arXiv Fulltext PDF:C\\:\\\\Users\\\\Jingtian Zhang\\\\Zotero\\\\storage\\\\KNJ2NF5G\\\\Lin \u7b49 - 2022 - ZIN When and How to Learn Invariance Without Envi.pdf:application/pdf;arXiv.org Snapshot:C\\:\\\\Users\\\\Jingtian Zhang\\\\Zotero\\\\storage\\\\77U6KCUK\\\\2203.html:text/html", "annote": "Comment: Accepted by NeurIPS 2022", "keywords": "Computer Science - Artificial Intelligence, Computer Science - Machine Learning", "note": "arXiv:2203.05818 [cs]", "year": "2022", "month": "October", "author": "Lin, Yong and Zhu, Shengyu and Tan, Lu and Cui, Peng", "publisher": "arXiv", "urldate": "2023-10-26", "abstract": "It is commonplace to encounter heterogeneous data, of which some aspects of the data distribution may vary but the underlying causal mechanisms remain constant. When data are divided into distinct environments according to the heterogeneity, recent invariant learning methods have proposed to learn robust and invariant models based on this environment partition. It is hence tempting to utilize the inherent heterogeneity even when environment partition is not provided. Unfortunately, in this work, we show that learning invariant features under this circumstance is fundamentally impossible without further inductive biases or additional information. Then, we propose a framework to jointly learn environment partition and invariant representation, assisted by additional auxiliary information. We derive sufficient and necessary conditions for our framework to provably identify invariant features under a fairly general setting. Experimental results on both synthetic and real world datasets validate our analysis and demonstrate an improved performance of the proposed framework over existing methods. Finally, our results also raise the need of making the role of inductive biases more explicit in future works, when considering learning invariant models without environment partition. Codes are available at https://github.com/linyongver/ZIN\\_official .", "url": "http://arxiv.org/abs/2203.05818", "shorttitle": "{ZIN}", "title": "ZIN: When and How to Learn Invariance Without Environment Partition?", "ENTRYTYPE": "misc", "ID": "lin_zin_2022"}, "6": {"file": "arXiv Fulltext PDF:C\\:\\\\Users\\\\Jingtian Zhang\\\\Zotero\\\\storage\\\\KB5LQK9Z\\\\Sun \u7b49 - 2023 - 3D-GPT Procedural 3D Modeling with Large Language.pdf:application/pdf;arXiv.org Snapshot:C\\:\\\\Users\\\\Jingtian Zhang\\\\Zotero\\\\storage\\\\8DX5DZR5\\\\2310.html:text/html", "annote": "Comment: Project page: https://chuny1.github.io/3DGPT/3dgpt.html", "keywords": "Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics", "note": "arXiv:2310.12945 [cs]", "year": "2023", "month": "October", "author": "Sun, Chunyi and Han, Junlin and Deng, Weijian and Wang, Xinlong and Qin, Zishan and Gould, Stephen", "publisher": "arXiv", "urldate": "2023-10-26", "abstract": "In the pursuit of efficient automated content creation, procedural generation, leveraging modifiable parameters and rule-based systems, emerges as a promising approach. Nonetheless, it could be a demanding endeavor, given its intricate nature necessitating a deep understanding of rules, algorithms, and parameters. To reduce workload, we introduce 3D-GPT, a framework utilizing large language models{\\textasciitilde}(LLMs) for instruction-driven 3D modeling. 3D-GPT positions LLMs as proficient problem solvers, dissecting the procedural 3D modeling tasks into accessible segments and appointing the apt agent for each task. 3D-GPT integrates three core agents: the task dispatch agent, the conceptualization agent, and the modeling agent. They collaboratively achieve two objectives. First, it enhances concise initial scene descriptions, evolving them into detailed forms while dynamically adapting the text based on subsequent instructions. Second, it integrates procedural generation, extracting parameter values from enriched text to effortlessly interface with 3D software for asset creation. Our empirical investigations confirm that 3D-GPT not only interprets and executes instructions, delivering reliable results but also collaborates effectively with human designers. Furthermore, it seamlessly integrates with Blender, unlocking expanded manipulation possibilities. Our work highlights the potential of LLMs in 3D modeling, offering a basic framework for future advancements in scene generation and animation.", "url": "http://arxiv.org/abs/2310.12945", "shorttitle": "{3D}-{GPT}", "title": "3D-GPT: Procedural 3D Modeling with Large Language Models", "ENTRYTYPE": "misc", "ID": "sun_3d-gpt_2023"}, "7": {"file": "arXiv Fulltext PDF:C\\:\\\\Users\\\\Jingtian Zhang\\\\Zotero\\\\storage\\\\BPRU6IAA\\\\Sun \u7b49 - 2023 - SALMON Self-Alignment with Principle-Following Re.pdf:application/pdf;arXiv.org Snapshot:C\\:\\\\Users\\\\Jingtian Zhang\\\\Zotero\\\\storage\\\\E3NMH79V\\\\2310.html:text/html", "annote": "Comment: Project page: https://github.com/IBM/SALMON", "keywords": "Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning", "note": "arXiv:2310.05910 [cs]", "year": "2023", "month": "October", "author": "Sun, Zhiqing and Shen, Yikang and Zhang, Hongxin and Zhou, Qinhong and Chen, Zhenfang and Cox, David and Yang, Yiming and Gan, Chuang", "publisher": "arXiv", "urldate": "2023-10-26", "abstract": "Supervised Fine-Tuning (SFT) on response demonstrations combined with Reinforcement Learning from Human Feedback (RLHF) constitutes a powerful paradigm for aligning LLM-based AI agents. However, a significant limitation of such an approach is its dependency on high-quality human annotations, making its application to intricate tasks challenging due to difficulties in obtaining consistent response demonstrations and in-distribution response preferences. This paper presents a novel approach, namely SALMON (Self-ALignMent with principle-fOllowiNg reward models), to align base language models with minimal human supervision, using only a small set of human-defined principles, yet achieving superior performance. Central to our approach is a principle-following reward model. Trained on synthetic preference data, this model can generate reward scores based on arbitrary human-defined principles. By merely adjusting these principles during the RL training phase, we gain full control over the preferences with the reward model, subsequently influencing the behavior of the RL-trained policies, and eliminating the reliance on the collection of online human preferences. Applying our method to the LLaMA-2-70b base language model, we developed an AI assistant named Dromedary-2. With only 6 exemplars for in-context learning and 31 human-defined principles, Dromedary-2 significantly surpasses the performance of several state-of-the-art AI systems, including LLaMA-2-Chat-70b, on various benchmark datasets. We have open-sourced the code and model weights to encourage further research into aligning LLM-based AI agents with enhanced supervision efficiency, improved controllability, and scalable oversight.", "url": "http://arxiv.org/abs/2310.05910", "shorttitle": "{SALMON}", "title": "SALMON: Self-Alignment with Principle-Following Reward Models", "ENTRYTYPE": "misc", "ID": "sun_salmon_2023"}, "8": {"file": "arXiv Fulltext PDF:C\\:\\\\Users\\\\Jingtian Zhang\\\\Zotero\\\\storage\\\\7RDJEBIS\\\\Liu \u7b49 - 2023 - HallusionBench You See What You Think Or You Thi.pdf:application/pdf;arXiv.org Snapshot:C\\:\\\\Users\\\\Jingtian Zhang\\\\Zotero\\\\storage\\\\DTJH6PZ3\\\\2310.html:text/html", "keywords": "Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition", "note": "arXiv:2310.14566 [cs]", "year": "2023", "month": "October", "author": "Liu, Fuxiao and Guan, Tianrui and Li, Zongxia and Chen, Lichang and Yacoob, Yaser and Manocha, Dinesh and Zhou, Tianyi", "publisher": "arXiv", "urldate": "2023-10-26", "abstract": "Large language models (LLMs), after being aligned with vision models and integrated into vision-language models (VLMs), can bring impressive improvement in image reasoning tasks. This was shown by the recently released GPT-4V(ison), LLaVA-1.5, etc. However, the strong language prior in these SOTA LVLMs can be a double-edged sword: they may ignore the image context and solely rely on the (even contradictory) language prior for reasoning. In contrast, the vision modules in VLMs are weaker than LLMs and may result in misleading visual representations, which are then translated to confident mistakes by LLMs. To study these two types of VLM mistakes, i.e., language hallucination and visual illusion, we curated HallusionBench, an image-context reasoning benchmark that is still challenging to even GPT-4V and LLaVA-1.5. We provide a detailed analysis of examples in HallusionBench, which sheds novel insights on the illusion or hallucination of VLMs and how to improve them in the future. The benchmark and codebase will be released at https://github.com/tianyi-lab/HallusionBench.", "url": "http://arxiv.org/abs/2310.14566", "shorttitle": "{HallusionBench}", "title": "HallusionBench: You See What You Think? Or You Think What You See? An Image-Context Reasoning Benchmark Challenging for GPT-4V(ision), LLaVA-1.5, and Other Multi-modality Models", "ENTRYTYPE": "misc", "ID": "liu_hallusionbench_2023"}, "9": {"file": "arXiv Fulltext PDF:C\\:\\\\Users\\\\Jingtian Zhang\\\\Zotero\\\\storage\\\\ABQT6ZXB\\\\Stechly \u7b49 - 2023 - GPT-4 Doesn't Know It's Wrong An Analysis of Iter.pdf:application/pdf;arXiv.org Snapshot:C\\:\\\\Users\\\\Jingtian Zhang\\\\Zotero\\\\storage\\\\Q6QPZ7P5\\\\2310.html:text/html", "annote": "Comment: 18 pages, 3 figures", "keywords": "Computer Science - Artificial Intelligence", "note": "arXiv:2310.12397 [cs]", "year": "2023", "month": "October", "author": "Stechly, Kaya and Marquez, Matthew and Kambhampati, Subbarao", "publisher": "arXiv", "urldate": "2023-10-26", "abstract": "There has been considerable divergence of opinion on the reasoning abilities of Large Language Models (LLMs). While the initial optimism that reasoning might emerge automatically with scale has been tempered thanks to a slew of counterexamples, a wide spread belief in their iterative self-critique capabilities persists. In this paper, we set out to systematically investigate the effectiveness of iterative prompting of LLMs in the context of Graph Coloring, a canonical NP-complete reasoning problem that is related to propositional satisfiability as well as practical problems like scheduling and allocation. We present a principled empirical study of the performance of GPT4 in solving graph coloring instances or verifying the correctness of candidate colorings. In iterative modes, we experiment with the model critiquing its own answers and an external correct reasoner verifying proposed solutions. In both cases, we analyze whether the content of the criticisms actually affects bottom line performance. The study seems to indicate that (i) LLMs are bad at solving graph coloring instances (ii) they are no better at verifying a solution--and thus are not effective in iterative modes with LLMs critiquing LLM-generated solutions (iii) the correctness and content of the criticisms--whether by LLMs or external solvers--seems largely irrelevant to the performance of iterative prompting. We show that the observed increase in effectiveness is largely due to the correct solution being fortuitously present in the top-k completions of the prompt (and being recognized as such by an external verifier). Our results thus call into question claims about the self-critiquing capabilities of state of the art LLMs.", "url": "http://arxiv.org/abs/2310.12397", "shorttitle": "{GPT}-4 {Doesn}'t {Know} {It}'s {Wrong}", "title": "GPT-4 Doesn't Know It's Wrong: An Analysis of Iterative Prompting for Reasoning Problems", "ENTRYTYPE": "misc", "ID": "stechly_gpt-4_2023"}, "10": {"file": "arXiv Fulltext PDF:C\\:\\\\Users\\\\Jingtian Zhang\\\\Zotero\\\\storage\\\\FDHAU9GK\\\\Valmeekam \u7b49 - 2023 - Can Large Language Models Really Improve by Self-c.pdf:application/pdf;arXiv.org Snapshot:C\\:\\\\Users\\\\Jingtian Zhang\\\\Zotero\\\\storage\\\\N6XZ7XEE\\\\2310.html:text/html", "keywords": "Computer Science - Artificial Intelligence", "note": "arXiv:2310.08118 [cs]", "year": "2023", "month": "October", "author": "Valmeekam, Karthik and Marquez, Matthew and Kambhampati, Subbarao", "publisher": "arXiv", "urldate": "2023-10-26", "abstract": "There have been widespread claims about Large Language Models (LLMs) being able to successfully verify or self-critique their candidate solutions in reasoning problems in an iterative mode. Intrigued by those claims, in this paper we set out to investigate the verification/self-critiquing abilities of large language models in the context of planning. We evaluate a planning system that employs LLMs for both plan generation and verification. We assess the verifier LLM's performance against ground-truth verification, the impact of self-critiquing on plan generation, and the influence of varying feedback levels on system performance. Using GPT-4, a state-of-the-art LLM, for both generation and verification, our findings reveal that self-critiquing appears to diminish plan generation performance, especially when compared to systems with external, sound verifiers and the LLM verifiers in that system produce a notable number of false positives, compromising the system's reliability. Additionally, the nature of feedback, whether binary or detailed, showed minimal impact on plan generation. Collectively, our results cast doubt on the effectiveness of LLMs in a self-critiquing, iterative framework for planning tasks.", "url": "http://arxiv.org/abs/2310.08118", "title": "Can Large Language Models Really Improve by Self-critiquing Their Own Plans?", "ENTRYTYPE": "misc", "ID": "valmeekam_can_2023"}, "11": {"file": "arXiv Fulltext PDF:C\\:\\\\Users\\\\Jingtian Zhang\\\\Zotero\\\\storage\\\\URV56SWR\\\\Xie \u7b49 - 2023 - OpenAgents An Open Platform for Language Agents i.pdf:application/pdf;arXiv.org Snapshot:C\\:\\\\Users\\\\Jingtian Zhang\\\\Zotero\\\\storage\\\\LT93LRDL\\\\2310.html:text/html", "annote": "Comment: 34 pages, 8 figures", "keywords": "Computer Science - Artificial Intelligence, Computer Science - Computation and Language", "note": "arXiv:2310.10634 [cs]", "year": "2023", "month": "October", "author": "Xie, Tianbao and Zhou, Fan and Cheng, Zhoujun and Shi, Peng and Weng, Luoxuan and Liu, Yitao and Hua, Toh Jing and Zhao, Junning and Liu, Qian and Liu, Che and Liu, Leo Z. and Xu, Yiheng and Su, Hongjin and Shin, Dongchan and Xiong, Caiming and Yu, Tao", "publisher": "arXiv", "urldate": "2023-10-26", "abstract": "Language agents show potential in being capable of utilizing natural language for varied and intricate tasks in diverse environments, particularly when built upon large language models (LLMs). Current language agent frameworks aim to facilitate the construction of proof-of-concept language agents while neglecting the non-expert user access to agents and paying little attention to application-level designs. We present OpenAgents, an open platform for using and hosting language agents in the wild of everyday life. OpenAgents includes three agents: (1) Data Agent for data analysis with Python/SQL and data tools; (2) Plugins Agent with 200+ daily API tools; (3) Web Agent for autonomous web browsing. OpenAgents enables general users to interact with agent functionalities through a web user interface optimized for swift responses and common failures while offering developers and researchers a seamless deployment experience on local setups, providing a foundation for crafting innovative language agents and facilitating real-world evaluations. We elucidate the challenges and opportunities, aspiring to set a foundation for future research and development of real-world language agents.", "url": "http://arxiv.org/abs/2310.10634", "shorttitle": "{OpenAgents}", "title": "OpenAgents: An Open Platform for Language Agents in the Wild", "ENTRYTYPE": "misc", "ID": "xie_openagents_2023"}, "12": {"file": "arXiv Fulltext PDF:C\\:\\\\Users\\\\Jingtian Zhang\\\\Zotero\\\\storage\\\\EEX22SUB\\\\Chen \u7b49 - 2023 - MiniGPT-v2 large language model as a unified inte.pdf:application/pdf;arXiv.org Snapshot:C\\:\\\\Users\\\\Jingtian Zhang\\\\Zotero\\\\storage\\\\TXN68TQM\\\\2310.html:text/html", "keywords": "Computer Science - Computer Vision and Pattern Recognition", "note": "arXiv:2310.09478 [cs]", "year": "2023", "month": "October", "author": "Chen, Jun and Zhu, Deyao and Shen, Xiaoqian and Li, Xiang and Liu, Zechun and Zhang, Pengchuan and Krishnamoorthi, Raghuraman and Chandra, Vikas and Xiong, Yunyang and Elhoseiny, Mohamed", "publisher": "arXiv", "urldate": "2023-10-26", "abstract": "Large language models have shown their remarkable capabilities as a general interface for various language-related applications. Motivated by this, we target to build a unified interface for completing many vision-language tasks including image description, visual question answering, and visual grounding, among others. The challenge is to use a single model for performing diverse vision-language tasks effectively with simple multi-modal instructions. Towards this objective, we introduce MiniGPT-v2, a model that can be treated as a unified interface for better handling various vision-language tasks. We propose using unique identifiers for different tasks when training the model. These identifiers enable our model to better distinguish each task instruction effortlessly and also improve the model learning efficiency for each task. After the three-stage training, the experimental results show that MiniGPT-v2 achieves strong performance on many visual question-answering and visual grounding benchmarks compared to other vision-language generalist models. Our model and codes are available at https://minigpt-v2.github.io/", "url": "http://arxiv.org/abs/2310.09478", "shorttitle": "{MiniGPT}-v2", "title": "MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning", "ENTRYTYPE": "misc", "ID": "chen_minigpt-v2_2023"}}}