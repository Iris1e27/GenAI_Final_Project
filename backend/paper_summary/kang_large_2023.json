{"summary": "In this paper, Kang et al. detail LIBRO, a framework to automate test generation from bug reports using Large Language Models (LLMs). LIBRO focus on post-processing steps to discern when LLMs are effective and rank the produced tests according to their validity. Results showed LIBRO was able to generate failure reproducing test cases for 33% of all studied cases, and likewise suggest a bug reproducing test in first place for 149 bugs. LIBRO was also tested on 31 bug reports submitted after LLM training data terminated, with it producing bug reproducing tests for 32% of the bug reports."}